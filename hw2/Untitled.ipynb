{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Problem 1 ##\n",
    "\n",
    "## part a ##\n",
    "\n",
    "import numpy as np\n",
    "import mltools as ml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.genfromtxt(\"data/curve80.txt\",delimiter=None) # load the text file\n",
    "\n",
    "##split data into 75% training data and 25% testing data\n",
    "## base predictions off training and test resulting prediction using testing data to verify correctness and minimize error\n",
    "\n",
    "X = data[:,0]\n",
    "X = X[:,np.newaxis] # code expects shape (M,N) so make sure it's 2-dimensional\n",
    "Y = data[:,1] # doesn't matter for Y\n",
    "Xtr,Xte,Ytr,Yte = ml.splitData(X,Y,0.75) # split data set 75/25\n",
    "\n",
    "## part b ##\n",
    "\n",
    "## train linear regress model and measure error against training and test MSE\n",
    "\n",
    "lr = ml.linear.linearRegress( Xtr, Ytr ) # create and train model\n",
    "xs = np.linspace(0,10,200) # densely sample possible x-values\n",
    "xs = xs[:,np.newaxis] # force \"xs\" to be an Mx1 matrix (expected by our code)\n",
    "ys = lr.predict( xs ) # make predictions at xs\n",
    "\n",
    "print \"Theta: \"\n",
    "print lr.theta[0][0]\n",
    "print\n",
    "plt.scatter(x=Xtr, y=Ytr)\n",
    "plt.plot(xs, ys)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "##e1 = [[x] for x in Ytr] - Xtr.dot( lr.theta )\n",
    "##trainingMSE = e1.T.dot(e1)/ np.mean(e1**2)\n",
    "print \"Training MSE: \"\n",
    "##print trainingMSE\n",
    "print lr.mse(Xtr, Ytr)\n",
    "print\n",
    "\n",
    "##e2 = [[x] for x in Yte] - Xte.dot( lr.theta )\n",
    "##testMSE = e2.T.dot(e2)/ np.mean(e2**2)\n",
    "print \"Test MSE: \"\n",
    "##print testMSE\n",
    "print lr.mse(Xte, Yte)\n",
    "print\n",
    "\n",
    "## part c ##\n",
    "## creates linear regression and measures MSE based on predictions\n",
    "## adds degrees to make function more dimensional\n",
    "\n",
    "d = [1, 3, 5, 7, 10, 18]\n",
    "errTest = []\n",
    "errTrain = []\n",
    "\n",
    "for degree in d:\n",
    "\n",
    "    ## expand and rescale features ##\n",
    "\n",
    "    # Create polynomial features up to \"degree\"; don't create constant feature\n",
    "    # (the linear regression learner will add the constant feature automatically)\n",
    "    XtrP = ml.transforms.fpoly(Xtr, degree, bias=False)\n",
    "    # Rescale the data matrix so that the features have similar ranges / variance\n",
    "    XtrP,params = ml.transforms.rescale(XtrP)\n",
    "    # \"params\" returns the transformation parameters (shift & scale)\n",
    "    # Then we can train the model on the scaled feature matrix:\n",
    "    lr = ml.linear.linearRegress( XtrP, Ytr ) # create and train model\n",
    "    # Now, apply the same polynomial expansion & scaling transformation to Xtest:\n",
    "    XteP,_ = ml.transforms.rescale( ml.transforms.fpoly(Xte,degree,False), params)\n",
    "\n",
    "    Phi = lambda X: ml.transforms.rescale(ml.transforms.fpoly(X, degree, False), params)[0]\n",
    "\n",
    "\n",
    "    ## make a set of points that are sorted by x, and are closely, linearly spaced\n",
    "    xs = np.linspace(0, 10, 200)  # densely sample possible x-values\n",
    "    xs = xs[:, np.newaxis]  # force \"xs\" to be an Mx1 matrix (expected by our code)\n",
    "\n",
    "\n",
    "    Xtr3 = Phi(xs)              ##transform the data\n",
    "    Ytr3 = lr.predict(Xtr3)     ##predict data (this is what we will graph because x is sorted and will display correctly)\n",
    "\n",
    "    YhatTrain = lr.predict(XtrP)  # predict on training data\n",
    "    YhatTest = lr.predict(XteP)  # predict on test data\n",
    "    # etc.\n",
    "\n",
    "    #e1 = [[x] for x in Ytr] - Xtr.dot(lr.theta)\n",
    "    #trainingMSE = e1.T.dot(e1) / np.mean(e1 ** 2)\n",
    "    errTest.append(lr.mse(XteP, Yte))\n",
    "    errTrain.append(lr.mse(XtrP, Ytr))\n",
    "\n",
    "    plt.plot(Xte, Yte, \"r.\")\n",
    "    ax = plt.axis()\n",
    "    plt.plot(Xtr, Ytr, \"g.\")\n",
    "    plt.plot(xs, Ytr3)\n",
    "    plt.axis(ax)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plt.semilogy(d, errTrain, color='blue')\n",
    "plt.semilogy(d, errTest, color='green') #TODO: \" \" to average and plot results on semi-log scale\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "## Problem 2 ##\n",
    "## grabs mean MSE error of linear regression based on cross validation folds\n",
    "\n",
    "J = []\n",
    "nFolds = 5\n",
    "E = []\n",
    "\n",
    "for degree in d:\n",
    "    errTi = []\n",
    "    errCrossTest = []\n",
    "    for iFold in range(nFolds):\n",
    "        XtrP = ml.transforms.fpoly(Xtr, degree, bias=False)\n",
    "        XteP = ml.transforms.fpoly(Xte, degree, bias=False)\n",
    "        Xti, Xvi, Yti, Yvi = ml.crossValidate(XtrP, Ytr, nFolds, iFold)\n",
    "        ##XtiP = ml.transforms.fpoly(Xti, degree, bias=False)\n",
    "        ##XtiP,params = ml.transforms.rescale(Xti)\n",
    "        # Rescale the data matrix so that the features have similar ranges / variance\n",
    "        learner = ml.linear.linearRegress(Xti,Yti)\n",
    "        errTi.append(learner.mse(Xvi, Yvi))\n",
    "        errCrossTest.append(learner.mse(XteP, Yte))\n",
    "    J.append(np.mean(errTi))\n",
    "    E.append(np.mean(errCrossTest))\n",
    "plt.show()\n",
    "plt.semilogy(d, J, color='blue') ## cross validation error\n",
    "plt.semilogy(d, E, color='red') ## testing error\n",
    "plt.close()\n",
    "##print J\n",
    "## Problem 1 ##\n",
    "\n",
    "## part a ##\n",
    "\n",
    "import numpy as np\n",
    "import mltools as ml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.genfromtxt(\"data/curve80.txt\",delimiter=None) # load the text file\n",
    "\n",
    "##split data into 75% training data and 25% testing data\n",
    "## base predictions off training and test resulting prediction using testing data to verify correctness and minimize error\n",
    "\n",
    "X = data[:,0]\n",
    "X = X[:,np.newaxis] # code expects shape (M,N) so make sure it's 2-dimensional\n",
    "Y = data[:,1] # doesn't matter for Y\n",
    "Xtr,Xte,Ytr,Yte = ml.splitData(X,Y,0.75) # split data set 75/25\n",
    "\n",
    "## part b ##\n",
    "\n",
    "## train linear regress model and measure error against training and test MSE\n",
    "\n",
    "lr = ml.linear.linearRegress( Xtr, Ytr ) # create and train model\n",
    "xs = np.linspace(0,10,200) # densely sample possible x-values\n",
    "xs = xs[:,np.newaxis] # force \"xs\" to be an Mx1 matrix (expected by our code)\n",
    "ys = lr.predict( xs ) # make predictions at xs\n",
    "\n",
    "print \"Theta: \"\n",
    "print lr.theta[0][0]\n",
    "print\n",
    "plt.scatter(x=Xtr, y=Ytr)\n",
    "plt.plot(xs, ys)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "##e1 = [[x] for x in Ytr] - Xtr.dot( lr.theta )\n",
    "##trainingMSE = e1.T.dot(e1)/ np.mean(e1**2)\n",
    "print \"Training MSE: \"\n",
    "##print trainingMSE\n",
    "print lr.mse(Xtr, Ytr)\n",
    "print\n",
    "\n",
    "##e2 = [[x] for x in Yte] - Xte.dot( lr.theta )\n",
    "##testMSE = e2.T.dot(e2)/ np.mean(e2**2)\n",
    "print \"Test MSE: \"\n",
    "##print testMSE\n",
    "print lr.mse(Xte, Yte)\n",
    "print\n",
    "\n",
    "## part c ##\n",
    "## creates linear regression and measures MSE based on predictions\n",
    "## adds degrees to make function more dimensional\n",
    "\n",
    "d = [1, 3, 5, 7, 10, 18]\n",
    "errTest = []\n",
    "errTrain = []\n",
    "\n",
    "for degree in d:\n",
    "\n",
    "    ## expand and rescale features ##\n",
    "\n",
    "    # Create polynomial features up to \"degree\"; don't create constant feature\n",
    "    # (the linear regression learner will add the constant feature automatically)\n",
    "    XtrP = ml.transforms.fpoly(Xtr, degree, bias=False)\n",
    "    # Rescale the data matrix so that the features have similar ranges / variance\n",
    "    XtrP,params = ml.transforms.rescale(XtrP)\n",
    "    # \"params\" returns the transformation parameters (shift & scale)\n",
    "    # Then we can train the model on the scaled feature matrix:\n",
    "    lr = ml.linear.linearRegress( XtrP, Ytr ) # create and train model\n",
    "    # Now, apply the same polynomial expansion & scaling transformation to Xtest:\n",
    "    XteP,_ = ml.transforms.rescale( ml.transforms.fpoly(Xte,degree,False), params)\n",
    "\n",
    "    Phi = lambda X: ml.transforms.rescale(ml.transforms.fpoly(X, degree, False), params)[0]\n",
    "\n",
    "\n",
    "    ## make a set of points that are sorted by x, and are closely, linearly spaced\n",
    "    xs = np.linspace(0, 10, 200)  # densely sample possible x-values\n",
    "    xs = xs[:, np.newaxis]  # force \"xs\" to be an Mx1 matrix (expected by our code)\n",
    "\n",
    "\n",
    "    Xtr3 = Phi(xs)              ##transform the data\n",
    "    Ytr3 = lr.predict(Xtr3)     ##predict data (this is what we will graph because x is sorted and will display correctly)\n",
    "\n",
    "    YhatTrain = lr.predict(XtrP)  # predict on training data\n",
    "    YhatTest = lr.predict(XteP)  # predict on test data\n",
    "    # etc.\n",
    "\n",
    "    #e1 = [[x] for x in Ytr] - Xtr.dot(lr.theta)\n",
    "    #trainingMSE = e1.T.dot(e1) / np.mean(e1 ** 2)\n",
    "    errTest.append(lr.mse(XteP, Yte))\n",
    "    errTrain.append(lr.mse(XtrP, Ytr))\n",
    "\n",
    "    plt.plot(Xte, Yte, \"r.\")\n",
    "    ax = plt.axis()\n",
    "    plt.plot(Xtr, Ytr, \"g.\")\n",
    "    plt.plot(xs, Ytr3)\n",
    "    plt.axis(ax)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plt.semilogy(d, errTrain, color='blue')\n",
    "plt.semilogy(d, errTest, color='green') #TODO: \" \" to average and plot results on semi-log scale\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "## Problem 2 ##\n",
    "## grabs mean MSE error of linear regression based on cross validation folds\n",
    "\n",
    "J = []\n",
    "nFolds = 5\n",
    "E = []\n",
    "\n",
    "for degree in d:\n",
    "    errTi = []\n",
    "    errCrossTest = []\n",
    "    for iFold in range(nFolds):\n",
    "        XtrP = ml.transforms.fpoly(Xtr, degree, bias=False)\n",
    "        XteP = ml.transforms.fpoly(Xte, degree, bias=False)\n",
    "        Xti, Xvi, Yti, Yvi = ml.crossValidate(XtrP, Ytr, nFolds, iFold)\n",
    "        ##XtiP = ml.transforms.fpoly(Xti, degree, bias=False)\n",
    "        ##XtiP,params = ml.transforms.rescale(Xti)\n",
    "        # Rescale the data matrix so that the features have similar ranges / variance\n",
    "        learner = ml.linear.linearRegress(Xti,Yti)\n",
    "        errTi.append(learner.mse(Xvi, Yvi))\n",
    "        errCrossTest.append(learner.mse(XteP, Yte))\n",
    "    J.append(np.mean(errTi))\n",
    "    E.append(np.mean(errCrossTest))\n",
    "plt.show()\n",
    "plt.semilogy(d, J, color='blue') ## cross validation error\n",
    "plt.semilogy(d, E, color='red') ## testing error\n",
    "plt.close()\n",
    "##print J\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
